# Chapter 1 Funmentals of Reinforcment Learning

> 本章的主要内容是强化学习的基础内容，包含一些基本的概念和方程的推导

## 学习笔记

### Week2
马尔可夫决策过程解决的问题：时序决策。即：智能体与环境的交互中会处于不同的状态，智能体在该状态会采取不同的动作值。一个时序决策的问题，就是一个马尔可夫决策过程。

主要的性质：马尔可夫性质，后续的回报和选择与仅与当前状态有关，与之前的状态无关。

强化学习的目标：在时序决策中最大化奖励，由于 *reward* 是概率性的，因此，实际上最大化的是奖励的合的期望。
学习片段：episode：由复位状态重新到状态结束的一个过程（以象棋举例来说是“新开一盘”）

实现AI可以归结为三种方式：
1. 人为地限定——即通过编码规定智能体该如何做；
2. 监督学习，给一定地数据集，让智能体学习如何做；
3. 强化学习，告诉智能体目标是什么，让智能体自己学会如何做；

强化学习中的奖励假设：

强化学习的过程可以抽象为最大化期望奖励的过程，描述这一过程主要有两个方面：1. 设计奖励；2. 最大化奖励算法。

关于奖励函数的设计，主要有以下几种：
1. 人为的指定、编码控制；
2. 从例子中学习，逆强化学习，让智能体从行为中学会奖励；
3. 优化：元强化学习，简而言之是多智能体的学习，然后从中获取对最终奖励有激励的行为(?)

奖励假设的不合理之处：
1. 奖励无法表示的东西：风险性；不同选择的差异性，如何保证探索；
2. 与人的高级行为相比，奖励是会变化的。

连续任务：

连续任务与离散任务相比有不同之处在于：
1. 连续任务不是片段化的，没有一个“终点”，将会与环境永远进行“互动”。这就决定了该奖励函数不会是一个有限的值，不能通过一个有限的时间序列来决定；(片段间是彼此独立的)。
2. 不能被划分成一个个eposide；

WEEK2-Summary:
1. 什么是MDP过程？
## 本章总结

## 习题解答

## 大作业

链接：
> https://zhangruochi.com/Bandits-and-Exploration-Exploitation/2020/09/03/

